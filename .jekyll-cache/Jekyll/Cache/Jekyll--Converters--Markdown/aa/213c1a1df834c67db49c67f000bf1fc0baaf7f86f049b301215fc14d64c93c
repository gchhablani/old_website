I"ð<h2 id="contributions">Contributions</h2>
<ul>
  <li>Gated Self-attention Memory Networks for Answer Selection
    <ul>
      <li>Builds upon two main ideas:
        <ol>
          <li>Gated-Attention Mechanism from GAReaders (Dhingra et al., 2017)
            <ul>
              <li><strong>Regular Attention</strong>:
  Given a context vector $ \mathbf{c} $ and an input sequence vector representation $X =  \begin{bmatrix}\mathbf{x_{1}\dots x_{n}}\end{bmatrix}$:</li>
            </ul>

\[\alpha_i = \frac{\exp{(\mathbf{c}^{T}\mathbf{x}_{j})}}{\sum_{j\in \begin{bmatrix} 1\dots n\end{bmatrix}}{\exp{(\mathbf{c}^{T}\mathbf{x}_{j})}}}\]
          </li>
        </ol>

        <ul>
          <li>
            <p><strong>Gated Attention</strong>: Instead of calculating a single attention value for each component and combining, make a vector for each.</p>

\[\mathbf{g}_{i} = \sigma(f(\mathbf{c},\mathbf{x}_{i}))\]
          </li>
          <li>
            <p><strong>Gated Self-Attention Mechanism</strong>(Proposed): The gating in gated attention is done based on context vector and single input vector. They want - self-attention + entire sequences as well.</p>
          </li>
        </ul>

\[\mathbf{v}^{j}  = \mathbf{W}\mathbf{x}_{j} + \mathbf{b} \; \mathbf{v}_{c}  = \mathbf{W}\mathbf{c} + \mathbf{b} \\\]

        <ol>
          <li>Memory-Networks (Sukhbaatar et al., 2015)</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h2 id="training">Training</h2>

<h3 id="hyperparameters">Hyperparameters</h3>
:ET