I"Œ<h2 id="contributions">Contributions</h2>

<ul>
  <li>
    <p>Gated Self-attention Memory Networks for Answer Selection</p>

    <ul>
      <li>Builds upon two main ideas:</li>
      <li>
        <p>Gated-Attention Mechanism from GAReaders (Dhingra et al., 2017)</p>

        <ul>
          <li><strong>Regular Attention</strong>: Given a context vector $ \mathbf{c} $ and an input sequence vector representation $X = \begin{bmatrix}\mathbf{x_{1}\dots x_{n}}\end{bmatrix}$:</li>
        </ul>

\[\alpha*{i} = \frac{\exp{(\mathbf{c}^{T}\mathbf{x}_{j})}}{\sum_{j\in \begin{bmatrix} 1\dots n\end{bmatrix}}{\exp{(\mathbf{c}^{T}\mathbf{x}_{j})}}}\]

        <ul>
          <li><strong>Gated Attention</strong>: Instead of calculating a single attention value for each component and combining, make a vector for each.</li>
        </ul>

        <p>\(\mathbf{g}_{i} = \sigma(f(\mathbf{c},\mathbf{x}_{i}))\) - <strong>Gated Self-Attention Mechanism</strong>(Proposed): The gating in gated attention is done based on context vector and single input vector. They want - self-attention + entire sequences as well.</p>
      </li>
    </ul>

\[\mathbf{v}^{j} = \mathbf{W}\mathbf{x}_{j} + \mathbf{b} ; \mathbf{v}^{c} = \mathbf{W}\mathbf{c} + \mathbf{b} \\

s^{j}_{i} = \mathbf{x}_{i}^{T}\mathbf{v}^{j}; s^{j}_{i} = \mathbf{x}_{i}^{T}\mathbf{v}^{c} \\

\alpha_{i}^{j} = \frac{\exp{(s_{i}^{j})}}{\sum_{k\in \begin{bmatrix} 1\dots n\end{bmatrix}}{\exp{(s_{i}^{k})}+\exp{(s_{i}^{c})}}}\\

\alpha_{i}^{c} = \frac{\exp{(s_{i}^{c})}}{\sum_{k\in \begin{bmatrix} 1\dots n\end{bmatrix}}{\exp{(s_{i}^{k})}+\exp{(s_{i}^{c})}}} \\

\begin{align*}
\mathbf{g}_{i} = &amp; f_{i}(c, X)\\
 = &amp; \sigma{\sum_{j}(\alpha_{i}^{j}\mathbf{x}^{j}) + \alpha_{i}^{c}\mathbf{c}}
    \end{align*}\]

    <ol>
      <li>Memory-Networks (Sukhbaatar et al., 2015)</li>
    </ol>
  </li>
</ul>

<h2 id="training">Training</h2>

<h3 id="hyperparameters">Hyperparameters</h3>
:ET