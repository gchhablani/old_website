I"z<h2 id="contributions">Contributions</h2>
<ul>
  <li>Gated Self-attention Memory Networks for Answer Selection
    <ul>
      <li>Combines two main ideas:
        <ol>
          <li>Gated-Attention Mechanism from GAReaders (Dhingra et al., 2017)
            <ul>
              <li>Regular attention works like so:
  Given a context vector</li>
            </ul>

\[\mathbf \\
 \alpha_i = \frac{\exp{(\mathbf{c^{T}x_{j}})}}{}\]
          </li>
          <li>Memory-Networks (Sukhbaatar et al., 2015)</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h2 id="training">Training</h2>

<h3 id="hyperparameters">Hyperparameters</h3>
:ET