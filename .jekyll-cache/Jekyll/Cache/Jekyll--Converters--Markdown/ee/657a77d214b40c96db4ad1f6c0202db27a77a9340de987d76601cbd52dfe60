I"+<h2 id="contributions">Contributions</h2>
<ul>
  <li>Gated Self-attention Memory Networks for Answer Selection
    <ul>
      <li>Builds upon two main ideas:
        <ol>
          <li>Gated-Attention Mechanism from GAReaders (Dhingra et al., 2017)
            <ul>
              <li><strong>Regular Attention</strong>:
  Given a context vector $ \mathbf{c} $ and an input sequence vector representation $X =  \begin{bmatrix}\mathbf{x_{1}\dots x_{n}}\end{bmatrix}$:</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
</ul>
:ET