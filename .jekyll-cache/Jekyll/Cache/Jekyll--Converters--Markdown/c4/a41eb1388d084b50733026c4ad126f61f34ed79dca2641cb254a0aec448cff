I"∞
<h2 id="contributions">Contributions</h2>
<ul>
  <li>
    <p>Gated Self-attention Memory Networks for Answer Selection</p>

    <ul>
      <li>Builds upon two main ideas:
        <ul>
          <li>
            <p>Gated-Attention Mechanism from GAReaders (Dhingra et al., 2017)</p>

            <ul>
              <li><strong>Regular Attention</strong>: Given a context vector $ \mathbf{c} $ and an input sequence vector representation $X = \begin{bmatrix}\mathbf{x_{1}\dots x_{n}}\end{bmatrix}$:</li>
            </ul>

\[\alpha*{i} = \frac{\exp{(\mathbf{c}^{T}\mathbf{x}_{j})}}{\sum_{j\in \begin{bmatrix} 1\dots n\end{bmatrix}}{\exp{(\mathbf{c}^{T}\mathbf{x}_{j})}}}\]

            <ul>
              <li><strong>Gated Attention</strong>: Instead of calculating a single attention value for each component and combining, make a vector for each.</li>
            </ul>

\[\mathbf{g}_{i} = \sigma(f(\mathbf{c},\mathbf{x}_{i}))\]

            <ul>
              <li><strong>Gated Self-Attention Mechanism</strong>(Proposed): The gating in gated attention is done based on context vector and single input vector. They want - self-attention + entire sequences as well. Hence, a different kind of vector is calculated as follows:</li>
            </ul>
          </li>
        </ul>

\[\mathbf{v}^{j} = \mathbf{W}\mathbf{x}_{j} + \mathbf{b} ; \mathbf{v}^{c} = \mathbf{W}\mathbf{c} + \mathbf{b} \\

s^{j}_{i} = \mathbf{x}_{i}^{T}\mathbf{v}^{j}; s^{j}_{i} = \mathbf{x}_{i}^{T}\mathbf{v}^{c} \\

\alpha_{i}^{j} = \frac{\exp{(s_{i}^{j})}}{\sum_{k\in \begin{bmatrix} 1\dots n\end{bmatrix}}{\exp{(s_{i}^{k})}+\exp{(s_{i}^{c})}}}\\

\alpha_{i}^{c} = \frac{\exp{(s_{i}^{c})}}{\sum_{k\in \begin{bmatrix} 1\dots n\end{bmatrix}}{\exp{(s_{i}^{k})}+\exp{(s_{i}^{c})}}} \\

\begin{align*}
\mathbf{g}_{i} = &amp; f_{i}(c, X)\\
 = &amp; \sigma{\sum_{j}(\alpha_{i}^{j}\mathbf{x}^{j}) + \alpha_{i}^{c}\mathbf{c}}
\end{align*}\]

        <ul>
          <li>Memory-Networks (Sukhbaatar et al., 2015)
            <ul>
              <li>They try to overcome the limitation of having a single control vector at every hop by combining GSAM with memory network architecture. Thus, creating the GSAMN. Let $\mathbf{k}$ denote the $k^{th}$ reasoning hop :</li>
            </ul>
          </li>
        </ul>

\[\mathbf{g}_{i} = f_{i}(\mathbf{c}_{k},X)\\

    \mathbf{x}_{i}^{k+1} = \mathbf{g}_{i} \odot \mathbf{x}_{i}^{k}\\\]
      </li>
    </ul>
  </li>
</ul>

<h2 id="training">Training</h2>

<h3 id="hyperparameters">Hyperparameters</h3>

<h2 id="doubts">Doubts</h2>
<ul>
  <li>Meaning of the sentence : ‚ÄúWe use affine-transformed inputs v and x
to calculate the self-attention instead of just x to
break the attention symmetry phenomenon.‚Äù (2.1)</li>
</ul>
:ET