---
layout: til
date: 2021-01-01
---
- Learned about Integrated Gradients [here](https://medium.com/@kartikeyabhardwaj98/integrated-gradients-for-deep-neural-networks-c114e3968eae) and [here](https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf). A useful technique for neural network interpretability. The second one was slightly tough to understand because of the TF code.

- Read and understood the about the first 600 lines of the code of transformers [BERT](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py) for a task. Thinking about writing a Medium Article when I'm done with the entire thing.

- Read short answer about PyTorch [`contiguous()`](https://stackoverflow.com/questions/48915810/pytorch-contiguous).

- Learned about Einstein Summation [here](https://en.wikipedia.org/wiki/Einstein_notation#:~:text=In%20mathematics%2C%20especially%20in%20applications,formula%2C%20thus%20achieving%20notational%20brevity.&text=It%20was%20introduced%20to%20physics%20by%20Albert%20Einstein%20in%201916.), and [here](https://pytorch.org/docs/stable/generated/torch.einsum.html) and [here](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html). Will write a short article soon.
